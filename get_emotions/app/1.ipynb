{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "path_to_photos = '/home/user/Documents/to_server/photos'\n",
    "path_to_data = '/home/user/Documents/to_server/abobus'\n",
    "path_to_yolo_emo = '/home/user/Documents/to_server/yolo_emo_val'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "emotions_dict =  {'Surprise':0,\n",
    "                 'Anger':1,\n",
    "                 'Sadness':2,\n",
    "                 'Disquietment':3,\n",
    "                 'Fear':4,\n",
    "                 'Peace':5,}\n",
    "# 'neutral':6}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['README_EMOTIC_annotationsStructure.pdf',\n 'Annotations.mat',\n 'annot_arrs_val.csv',\n 'annot_arrs_extra_train.csv',\n 'annot_arrs_train.csv',\n 'class_weights.csv',\n 'annot_arrs_test.csv']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path_to_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                        Filename  Width  Height    Age  Gender  Valence  \\\n0         h616x5yhky60how3nt.jpg  317.0   380.0  Adult  Female      3.4   \n1         h616x5yhky60how3nt.jpg  317.0   380.0  Adult    Male      4.0   \n2  COCO_val2014_000000020459.jpg  640.0   458.0  Adult    Male      4.4   \n3  COCO_val2014_000000033835.jpg  640.0   480.0  Adult    Male      6.2   \n4         arhnymaeequumexga8.jpg  267.0   400.0  Adult    Male      4.8   \n\n   Arousal  Dominance  Peace  Affection  ...  Disquietment  Fear  Pain  \\\n0      6.0        5.8    0.0        0.0  ...           1.0   0.0   0.0   \n1      5.0        4.4    0.0        0.0  ...           0.0   1.0   0.0   \n2      8.4        5.0    0.0        0.0  ...           0.0   0.0   0.0   \n3      4.2        7.4    0.0        1.0  ...           0.0   0.0   0.0   \n4      6.0        6.2    1.0        0.0  ...           1.0   0.0   0.0   \n\n   Suffering  X_min  Y_min  X_max  Y_max           Arr_name  \\\n0        0.0  146.0   76.0  379.0  317.0  img_arr_17077.npy   \n1        0.0   37.0   35.0  192.0  317.0  img_arr_17077.npy   \n2        0.0  109.0  108.0  401.0  450.0  img_arr_17078.npy   \n3        0.0  404.0  222.0  563.0  473.0  img_arr_17079.npy   \n4        0.0    2.0    2.0  370.0  266.0  img_arr_17080.npy   \n\n            Crop_name  \n0  crop_arr_val_0.npy  \n1  crop_arr_val_1.npy  \n2  crop_arr_val_2.npy  \n3  crop_arr_val_3.npy  \n4  crop_arr_val_4.npy  \n\n[5 rows x 40 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Width</th>\n      <th>Height</th>\n      <th>Age</th>\n      <th>Gender</th>\n      <th>Valence</th>\n      <th>Arousal</th>\n      <th>Dominance</th>\n      <th>Peace</th>\n      <th>Affection</th>\n      <th>...</th>\n      <th>Disquietment</th>\n      <th>Fear</th>\n      <th>Pain</th>\n      <th>Suffering</th>\n      <th>X_min</th>\n      <th>Y_min</th>\n      <th>X_max</th>\n      <th>Y_max</th>\n      <th>Arr_name</th>\n      <th>Crop_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>h616x5yhky60how3nt.jpg</td>\n      <td>317.0</td>\n      <td>380.0</td>\n      <td>Adult</td>\n      <td>Female</td>\n      <td>3.4</td>\n      <td>6.0</td>\n      <td>5.8</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>146.0</td>\n      <td>76.0</td>\n      <td>379.0</td>\n      <td>317.0</td>\n      <td>img_arr_17077.npy</td>\n      <td>crop_arr_val_0.npy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>h616x5yhky60how3nt.jpg</td>\n      <td>317.0</td>\n      <td>380.0</td>\n      <td>Adult</td>\n      <td>Male</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>37.0</td>\n      <td>35.0</td>\n      <td>192.0</td>\n      <td>317.0</td>\n      <td>img_arr_17077.npy</td>\n      <td>crop_arr_val_1.npy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>COCO_val2014_000000020459.jpg</td>\n      <td>640.0</td>\n      <td>458.0</td>\n      <td>Adult</td>\n      <td>Male</td>\n      <td>4.4</td>\n      <td>8.4</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>109.0</td>\n      <td>108.0</td>\n      <td>401.0</td>\n      <td>450.0</td>\n      <td>img_arr_17078.npy</td>\n      <td>crop_arr_val_2.npy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>COCO_val2014_000000033835.jpg</td>\n      <td>640.0</td>\n      <td>480.0</td>\n      <td>Adult</td>\n      <td>Male</td>\n      <td>6.2</td>\n      <td>4.2</td>\n      <td>7.4</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>404.0</td>\n      <td>222.0</td>\n      <td>563.0</td>\n      <td>473.0</td>\n      <td>img_arr_17079.npy</td>\n      <td>crop_arr_val_3.npy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>arhnymaeequumexga8.jpg</td>\n      <td>267.0</td>\n      <td>400.0</td>\n      <td>Adult</td>\n      <td>Male</td>\n      <td>4.8</td>\n      <td>6.0</td>\n      <td>6.2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>370.0</td>\n      <td>266.0</td>\n      <td>img_arr_17080.npy</td>\n      <td>crop_arr_val_4.npy</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 40 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_val = pd.read_csv(f'{path_to_data}/annot_arrs_val.csv')\n",
    "arr_val.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Filename', 'Width', 'Height', 'Age', 'Gender', 'Valence', 'Arousal',\n       'Dominance', 'Peace', 'Affection', 'Esteem', 'Anticipation',\n       'Engagement', 'Confidence', 'Happiness', 'Pleasure', 'Excitement',\n       'Surprise', 'Sympathy', 'Doubt/Confusion', 'Disconnection', 'Fatigue',\n       'Embarrassment', 'Yearning', 'Disapproval', 'Aversion', 'Annoyance',\n       'Anger', 'Sensitivity', 'Sadness', 'Disquietment', 'Fear', 'Pain',\n       'Suffering', 'X_min', 'Y_min', 'X_max', 'Y_max', 'Arr_name',\n       'Crop_name'],\n      dtype='object')"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_val.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['Surprise', 'Anger', 'Sadness', 'Disquietment', 'Fear', 'Peace'])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "biggest_emotion = arr_val.iloc[:][emotions_dict.keys()].idxmax(axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "'crop_arr_val_0.png'"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_val['Crop_name'][0].replace('npy', 'png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "def find_emotion(idx):\n",
    "    return emotions_dict[biggest_emotion[idx]]\\\n",
    "        if not len(set(arr_val.iloc[0][emotions_dict.keys()])) else len(emotions_dict.keys())\n",
    "\n",
    "def bigger_than_one(val):\n",
    "    if val > 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "def create_file(data, path, mode='w'):\n",
    "    with open(f'{path}', mode) as f:\n",
    "        f.write(f'{data}\\n')\n",
    "\n",
    "def bbox_to_yolo(data, n_class=0, w=1280, h=720):\n",
    "        # TODO add width, height\n",
    "        \"\"\"\n",
    "        translate bbox coordinates to yolo format\n",
    "        \"\"\"\n",
    "        # x1, y1 = data['x1'], data['y1']\n",
    "        # x2, y2 = data['x2'], data['y2']\n",
    "        x1, y1 = data[0], data[1]\n",
    "        x2, y2 = data[2], data[3]\n",
    "\n",
    "        yolo_x = bigger_than_one((int(abs(x1 + x2)) / 2) / w)\n",
    "        yolo_y = bigger_than_one((int(abs(y1 + y2)) / 2) / h)\n",
    "        yolo_w = bigger_than_one(int(abs(x1 - x2)) / w)\n",
    "        yolo_h = bigger_than_one(int(abs(y1 - y2)) / h)\n",
    "\n",
    "        return [n_class, yolo_x, yolo_y, yolo_w, yolo_h]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "for idx in range(100):#len(arr_val)):\n",
    "    shutil.copy(f\"{path_to_photos}/{arr_val['Crop_name'][idx].replace('npy', 'png')}\",\n",
    "                f\"{path_to_yolo_emo}\")\n",
    "    create_file(data=\" \".join([str(x) for x in bbox_to_yolo([arr_val.iloc[idx]['X_min'], arr_val.iloc[idx]['Y_min'],\n",
    "                      arr_val.iloc[idx]['X_max'], arr_val.iloc[idx]['Y_max']],\n",
    "                     n_class=find_emotion(idx),\n",
    "                     w=arr_val.iloc[idx]['Width'], h=arr_val.iloc[idx]['Height'])]),\n",
    "                path=f\"{path_to_yolo_emo}/{arr_val['Crop_name'][idx].replace('npy', 'txt')}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "'1 0.8280757097791798 0.5171052631578947 0.7350157728706624 0.6342105263157894'"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range():\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/user/Documents/to_server/yolo_emo_val/crop_arr_val_99.txt'"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
